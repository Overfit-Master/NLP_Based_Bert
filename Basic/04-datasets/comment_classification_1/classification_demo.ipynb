{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 文本分类实例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7766\n",
      "7765\n"
     ]
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['label', 'review'],\n        num_rows: 6988\n    })\n    test: Dataset({\n        features: ['label', 'review'],\n        num_rows: 777\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过新方法加载数据和数据清洗\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "print(len(dataset))\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "print(len(dataset))\n",
    "# 划分数据集\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "split_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/6988 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40d390714316410b87badf5525548f2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/777 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0907030db4e46f5af03724403484478"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 6988\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 777\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行分词\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "def process_function(example):\n",
    "    tokenized_example = tokenizer(example[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_example[\"labels\"] = example[\"label\"]\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_datasets = split_dataset.map(function=process_function, batched=True, remove_columns=split_dataset[\"train\"].column_names)\n",
    "\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 创建dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "\n",
    "valid_loader = DataLoader(tokenized_datasets[\"test\"], batch_size=64, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n        [ 101,  868,  711,  ...,    0,    0,    0],\n        [ 101, 3302, 1218,  ...,    0,    0,    0],\n        ...,\n        [ 101, 4692, 6814,  ...,  817, 2347,  102],\n        [ 101, 1343, 5722,  ..., 2791, 7313,  102],\n        [ 101, 2137, 4638,  ..., 6574, 7030,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n        0, 0, 1, 1, 1, 1, 1, 1])}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(train_loader))[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "\n",
    "# 模型及优化器定义\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "optimizer = Adam(model.parameters(), lr=0.00002)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(epochs, log_step=10):\n",
    "    for epoch in range(epochs):\n",
    "        step=0\n",
    "        model.train()\n",
    "        print(f\"<<<<<<<<Training on epoch{epoch} >>>>>>>>\")\n",
    "        for batch in train_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % log_step == 0:\n",
    "                print(f\"global_step: {step}, loss: {output.loss.item()}\")\n",
    "            step += 1\n",
    "        evaluate()\n",
    "\n",
    "\n",
    "# 模型验证\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    acc_num = 0\n",
    "    total = 0\n",
    "    # 该方法比no_grad()更加的优化了测试的内存管理方面\n",
    "    with torch.inference_mode():\n",
    "        for batch in valid_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "                output = model(**batch)\n",
    "                pred = torch.argmax(output.logits, dim=-1)\n",
    "                for p, l in zip(pred, batch[\"labels\"]):\n",
    "                    total += 1\n",
    "                    if p == l:\n",
    "                        acc_num += 1\n",
    "        print(f\"acc: {acc_num / total}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<Training on epoch0 >>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31825\\.conda\\envs\\transformers\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 0, loss: 0.6127764582633972\n",
      "global_step: 10, loss: 0.7215803265571594\n",
      "global_step: 20, loss: 0.488822340965271\n",
      "global_step: 30, loss: 0.5125473141670227\n",
      "global_step: 40, loss: 0.5366478562355042\n",
      "global_step: 50, loss: 0.39659732580184937\n",
      "global_step: 60, loss: 0.3305644094944\n",
      "global_step: 70, loss: 0.38386204838752747\n",
      "global_step: 80, loss: 0.19288167357444763\n",
      "global_step: 90, loss: 0.33696407079696655\n",
      "global_step: 100, loss: 0.2079067975282669\n",
      "global_step: 110, loss: 0.257258802652359\n",
      "global_step: 120, loss: 0.2732982933521271\n",
      "global_step: 130, loss: 0.25069162249565125\n",
      "global_step: 140, loss: 0.4484230577945709\n",
      "global_step: 150, loss: 0.14048022031784058\n",
      "global_step: 160, loss: 0.2136348932981491\n",
      "global_step: 170, loss: 0.22057226300239563\n",
      "global_step: 180, loss: 0.4068312644958496\n",
      "global_step: 190, loss: 0.15010777115821838\n",
      "global_step: 200, loss: 0.17918621003627777\n",
      "global_step: 210, loss: 0.3158644735813141\n",
      "acc: 0.8764478764478765\n",
      "<<<<<<<<Training on epoch1 >>>>>>>>\n",
      "global_step: 0, loss: 0.14867918193340302\n",
      "global_step: 10, loss: 0.14167340099811554\n",
      "global_step: 20, loss: 0.1999424248933792\n",
      "global_step: 30, loss: 0.4466370642185211\n",
      "global_step: 40, loss: 0.2592410147190094\n",
      "global_step: 50, loss: 0.30070292949676514\n",
      "global_step: 60, loss: 0.12001509964466095\n",
      "global_step: 70, loss: 0.12126033008098602\n",
      "global_step: 80, loss: 0.2221679836511612\n",
      "global_step: 90, loss: 0.17804208397865295\n",
      "global_step: 100, loss: 0.5364956855773926\n",
      "global_step: 110, loss: 0.20220567286014557\n",
      "global_step: 120, loss: 0.3657405376434326\n",
      "global_step: 130, loss: 0.2961367070674896\n",
      "global_step: 140, loss: 0.2626267671585083\n",
      "global_step: 150, loss: 0.21813437342643738\n",
      "global_step: 160, loss: 0.4404258131980896\n",
      "global_step: 170, loss: 0.2235713005065918\n",
      "global_step: 180, loss: 0.3369399905204773\n",
      "global_step: 190, loss: 0.25226715207099915\n",
      "global_step: 200, loss: 0.3622224032878876\n",
      "global_step: 210, loss: 0.07957462966442108\n",
      "acc: 0.8918918918918919\n",
      "<<<<<<<<Training on epoch2 >>>>>>>>\n",
      "global_step: 0, loss: 0.13384145498275757\n",
      "global_step: 10, loss: 0.19723857939243317\n",
      "global_step: 20, loss: 0.1029515340924263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epochs, log_step)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m----> 9\u001B[0m         batch \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mcuda() \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     11\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch)\n",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m----> 9\u001B[0m         batch \u001B[38;5;241m=\u001B[39m {k: \u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     11\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "test = \"这家酒店不错，饭很好吃\"\n",
    "with torch.inference_mode():\n",
    "    test_tensor = tokenizer(test, return_tensors=\"pt\")\n",
    "    test_tensor = {k: v.cuda() for k, v, in test_tensor.items()}\n",
    "    test_result = model(**test_tensor).logits\n",
    "    pred = torch.argmax(test_result, dim=-1)\n",
    "    print(pred.item())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 创建pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'LABEL_1', 'score': 0.9989665746688843}]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"这家酒店不错，饭很好吃\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
